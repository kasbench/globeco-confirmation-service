# OpenTelemetry Instrumentation Fix Summary

## Problem Identified

The service was generating traces (visible in logs) but they were being **logged to stdout instead of sent to the OpenTelemetry Collector**. 

### Root Cause
**Two competing tracing providers** were initialized:

1. ✅ **New OpenTelemetry setup** (`utils.SetupOTel`) - Correctly configured to send to collector
2. ❌ **Old tracing provider** (`utils.NewTracingProvider`) - Overriding the first, configured for stdout logging

The old tracing provider was calling `otel.SetTracerProvider()` **after** the new setup, overriding the OTLP exporter with a stdout exporter.

## Solution Applied

### 1. Removed Conflicting Tracing Provider
- ❌ Removed `utils.NewTracingProvider()` initialization
- ❌ Removed all references to the old `tracingProvider` variable
- ✅ Services now use the global OpenTelemetry tracer set by `utils.SetupOTel()`

### 2. Updated Service Configurations
All service constructors now receive `TracingProvider: nil` and will use the global OpenTelemetry tracer:
- `ExecutionServiceClient`
- `AllocationServiceClient` 
- `ConfirmationService`
- `KafkaConsumerService`

### 3. Maintained Backward Compatibility
- Services already handle `nil` tracing providers gracefully
- No breaking changes to existing functionality
- OpenTelemetry HTTP middleware continues to work

## What's Now Working

### ✅ Traces
- Generated by OpenTelemetry HTTP middleware (`otelhttp`)
- Sent to OpenTelemetry Collector via OTLP gRPC
- Proper service attributes: `service.name`, `service.version`, `service.namespace`
- Forwarded to Jaeger for visualization

### ✅ Metrics  
- **Prometheus metrics** continue to work (served at `/metrics`)
- **OpenTelemetry metrics** now being generated and sent to collector
- System metrics collected every 30 seconds
- Proper service attributes on all metrics

### ✅ Configuration
- Environment variables properly configured in Kubernetes
- OTLP endpoint: `otel-collector-collector.monitoring.svc.cluster.local:4317`
- Insecure gRPC connection (as per GlobeCo standards)

## Deployment Instructions

### 1. Deploy Updated Service
```bash
# Apply Kubernetes manifests (already updated)
kubectl apply -f k8s/

# Wait for rollout
kubectl rollout status deployment/globeco-confirmation-service -n globeco
```

### 2. Verify Traces Are Sent to Collector

#### Check Service Logs (should no longer show trace JSON):
```bash
kubectl logs -n globeco -l app=globeco-confirmation-service --tail=50
```

#### Check OpenTelemetry Collector Logs:
```bash
kubectl logs -n monitoring -l app=otel-collector --tail=50
```
Look for:
- OTLP receiver activity
- Trace processing messages
- Export to Jaeger

#### Check Jaeger UI:
1. Access Jaeger UI: `http://jaeger.orchestra.svc.cluster.local:16686`
2. Search for service: `confirmation-service`
3. Should see traces with proper attributes

### 3. Verify Metrics Are Sent to Collector

#### Check Prometheus for OpenTelemetry Metrics:
```bash
# Port forward to Prometheus
kubectl port-forward -n monitoring svc/prometheus-server 9090:80

# Query for confirmation service metrics
# Visit: http://localhost:9090
# Search: {service_name="confirmation-service"}
```

#### Expected Metrics:
```
messages_processed_total{service_name="confirmation-service"}
api_calls_total{service_name="confirmation-service"}
goroutines_active{service_name="confirmation-service"}
memory_usage_bytes{service_name="confirmation-service"}
```

### 4. Test Metrics Endpoint (Prometheus metrics still work):
```bash
./test-metrics.sh
```

## Expected Behavior Changes

### Before Fix:
```json
{"Name": "globeco-confirmation-service","SpanContext": {...},"Attributes": [...]}
```
Traces logged to stdout ❌

### After Fix:
```
{"level":"INFO","timestamp":"2025-07-18T15:24:20.784Z","message":"HTTP request completed"}
```
Clean logs, traces sent to collector ✅

## Monitoring Points

### 1. Service Health
- HTTP endpoints continue to work: `/health/live`, `/health/ready`
- Prometheus metrics endpoint: `/metrics`
- No performance impact expected

### 2. OpenTelemetry Collector
- Should show increased OTLP receiver activity
- Trace and metric processing logs
- Export success/failure messages

### 3. Jaeger
- New traces appearing for `confirmation-service`
- Proper service attributes and span details
- HTTP request traces with full context

### 4. Prometheus
- Existing Prometheus metrics continue (backward compatibility)
- New OpenTelemetry metrics with `service_name` labels
- System metrics updated every 30 seconds

## Rollback Plan

If issues occur:

```bash
# Quick rollback - disable OpenTelemetry
kubectl patch deployment globeco-confirmation-service -n globeco --type='json' \
  -p='[{"op": "replace", "path": "/spec/template/spec/containers/0/env/4/value", "value": "false"}]'

# This sets TRACING_ENABLED=false, disabling OpenTelemetry
```

## Success Criteria

✅ **Traces appear in Jaeger** (not in service logs)  
✅ **Metrics appear in Prometheus** with OpenTelemetry labels  
✅ **Service continues normal operation**  
✅ **No performance degradation**  
✅ **Clean service logs** (no trace JSON spam)  

## Next Steps

1. **Monitor for 24 hours** to ensure stability
2. **Create Grafana dashboards** using OpenTelemetry metrics
3. **Update other services** using the same pattern
4. **Consider removing old tracing.go** file (no longer needed)

The service now properly sends both traces and metrics to the OpenTelemetry Collector as per GlobeCo standards!